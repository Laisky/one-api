package aws

// Request represents a chat completion request to AWS Bedrock OpenAI OSS models.
//
// This structure defines all the parameters needed to send a chat completion
// request to OpenAI OSS models ([gpt-oss-20b], [gpt-oss-120b]) via AWS Bedrock.
// These models support reasoning capabilities similar to DeepSeek-R1.
//
// Based on AWS Bedrock OpenAI OSS documentation.
//
// [gpt-oss-20b]: https://openai.com/index/introducing-gpt-oss/
// [gpt-oss-120b]: https://openai.com/index/introducing-gpt-oss/
type Request struct {
	// Messages contains the conversation history including system, user, and assistant messages.
	// This field is required and must contain at least one message.
	// OpenAI OSS models process these messages to generate responses with reasoning content.
	Messages []Message `json:"messages"`

	// MaxTokens specifies the maximum number of tokens to generate in the response.
	// Optional field that helps control response length and API costs.
	// OpenAI OSS models may use additional tokens for reasoning content.
	MaxTokens int `json:"max_tokens,omitempty"`

	// Temperature controls the randomness of the model's responses.
	// Range: 0.0 to 1.0, where 0.0 is deterministic and 1.0 is most random.
	// Optional field, uses model default if not specified.
	Temperature *float64 `json:"temperature,omitempty"`

	// TopP controls nucleus sampling, limiting the cumulative probability of token choices.
	// Range: 0.0 to 1.0, where lower values make responses more focused.
	// Optional field, uses model default if not specified.
	TopP *float64 `json:"top_p,omitempty"`

	// Stop contains custom strings that will stop generation when encountered.
	// Optional field that allows fine-grained control over response termination.
	// Useful for controlling when OpenAI OSS models stop generating reasoning or response content.
	Stop []string `json:"stop,omitempty"`
}

// Message represents a single message in the conversation history.
//
// Messages form the core of the chat completion request, containing the back-and-forth
// conversation between different participants. Each message has a specific role that
// determines its purpose and expected content format for OpenAI OSS processing.
type Message struct {
	// Role identifies the sender of the message.
	// Valid values: "system" (instructions to guide model behavior),
	// "user" (human input), "assistant" (model response with potential reasoning content).
	Role string `json:"role"`

	// Content contains the text content of the message.
	// Required for system and user messages. For assistant messages, this contains
	// the final response text after reasoning (if any) has been completed.
	Content string `json:"content,omitempty"`
}

// Response represents the complete response from AWS Bedrock OpenAI OSS models.
//
// This structure contains the model's response to a chat completion request,
// including generated text and potentially reasoning content. OpenAI OSS responses
// may include both the reasoning process and the final answer in a structured format.
//
// Based on AWS Bedrock OpenAI OSS documentation and OpenAI API format.
type Response struct {
	// ID is a unique identifier for this chat completion.
	ID string `json:"id"`

	// Object identifies the type of response, always "chat.completion".
	Object string `json:"object"`

	// Created is the Unix timestamp of when the completion was created.
	Created int64 `json:"created"`

	// Model identifies which model was used to generate the response.
	Model string `json:"model"`

	// Choices contains the possible response options generated by the model.
	// Typically contains a single choice, but the array format maintains
	// compatibility with OpenAI's API structure. Each choice may include reasoning content.
	Choices []Choice `json:"choices"`

	// Usage contains information about token consumption for billing purposes.
	Usage Usage `json:"usage"`
}

// Choice represents a single response option from the OpenAI OSS model.
//
// Each choice contains the generated content along with metadata about
// why the generation stopped. OpenAI OSS choices may include reasoning
// content that shows the model's thought process before arriving at the final answer.
type Choice struct {
	// Index identifies the position of this choice in the choices array.
	// Typically 0 for the first (and usually only) choice.
	Index int `json:"index"`

	// Message contains the actual response content from the assistant.
	// This includes the final response text and may reference reasoning content
	// that was generated during the model's thought process.
	Message ResponseMessage `json:"message"`

	// FinishReason indicates why the model stopped generating tokens.
	// Valid values: "stop" (natural completion), "length" (max tokens reached),
	// "content_filter" (content filtered), etc.
	FinishReason string `json:"finish_reason"`
}

// ResponseMessage represents the assistant's message in the response.
//
// This structure contains the model's generated content. For OpenAI OSS models,
// this typically includes the final answer after any reasoning process has
// been completed. The role is always "assistant" for response messages.
type ResponseMessage struct {
	// Role identifies the message sender, always "assistant" for responses.
	// This maintains consistency with the conversation message format.
	Role string `json:"role"`

	// Content contains the generated text response from the model.
	// This is the final answer or response after OpenAI OSS reasoning
	// process (if any) has been completed.
	Content string `json:"content,omitempty"`
}

// Usage represents token consumption information for billing purposes.
//
// This structure provides detailed information about how many tokens
// were consumed during the request processing, which is essential
// for accurate billing and cost tracking.
type Usage struct {
	// InputTokens represents the number of tokens in the input prompt.
	// This includes all messages in the conversation history.
	InputTokens int `json:"prompt_tokens"`

	// OutputTokens represents the number of tokens generated in the response.
	// This includes reasoning content and the final answer.
	OutputTokens int `json:"completion_tokens"`

	// TotalTokens is the sum of InputTokens and OutputTokens.
	// Used for billing calculations and API usage tracking.
	TotalTokens int `json:"total_tokens"`
}

// StreamResponse represents a single chunk in a streaming response from OpenAI OSS models.
//
// When using streaming mode, the model sends multiple StreamResponse chunks
// instead of a single complete Response. Each chunk contains incremental
// updates (deltas) that should be accumulated to build the complete response,
// including any reasoning content that OpenAI OSS models generate.
type StreamResponse struct {
	// ID is a unique identifier for this streaming chat completion.
	ID string `json:"id"`

	// Object identifies the type of response, always "chat.completion.chunk".
	Object string `json:"object"`

	// Created is the Unix timestamp of when the completion was created.
	Created int64 `json:"created"`

	// Model identifies which model was used to generate the response.
	Model string `json:"model"`

	// Choices contains the streaming choice updates for this chunk.
	// Typically contains a single choice with delta information
	// representing the incremental content since the last chunk.
	Choices []StreamChoice `json:"choices"`
}

// StreamChoice represents a single streaming choice update.
//
// Each StreamChoice contains delta information that represents
// the incremental changes since the previous chunk. This allows
// clients to build up the complete response progressively,
// including reasoning content from OpenAI OSS models.
type StreamChoice struct {
	// Index identifies the position of this choice in the choices array.
	// Typically 0 for the first (and usually only) choice.
	Index int `json:"index"`

	// Delta contains the incremental update for this chunk.
	// This represents the new content that has been generated
	// since the previous chunk was sent.
	Delta StreamResponseMessage `json:"delta"`

	// FinishReason indicates why the model stopped generating tokens.
	// This field is only present in the final chunk of the stream.
	// Valid values: "stop", "length", "content_filter", etc.
	FinishReason string `json:"finish_reason,omitempty"`
}

// StreamResponseMessage represents the delta content in a streaming response.
//
// This structure contains the incremental content updates that should
// be appended to build the complete response. For OpenAI OSS models,
// this includes both reasoning content deltas and final answer deltas.
type StreamResponseMessage struct {
	// Role identifies the message sender, typically "assistant" for responses.
	// This field is usually only present in the first chunk of a stream.
	Role string `json:"role,omitempty"`

	// Content contains the incremental text content for this chunk.
	// This should be appended to previous chunks to build the complete response.
	Content string `json:"content,omitempty"`
}
